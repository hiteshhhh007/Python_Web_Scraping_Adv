{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#Requests Library for downloading webpages from the web\n",
    "url='https://github.com/topics'\n",
    "response=requests.get(url)\n",
    "#.get() is a method of requests library which is used to download the webpage.\n",
    "response.status_code\n",
    "#This will give the status code of the webpage and if it is 200 then it means that the webpage is downloaded successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content=response.text\n",
    "html_content\n",
    "#This will give the html code of the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('webpage.html','w',encoding=\"utf-8\") as f:\n",
    "    f.write(html_content)\n",
    "#This will create a html file of the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup=BeautifulSoup(html_content,'html.parser')\n",
    "#This will parse the html code of the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_tags_all=soup.find_all('p')\n",
    "#This will give the list of all the p tags in the webpage\n",
    "\n",
    "p_name=soup.find_all('p',class_='f3 lh-condensed mb-0 mt-1 Link--primary')\n",
    "print(p_name)\n",
    "#This will give the list of all the p tags in the webpage whose class is f3 lh-condensed mb-0 mt-1 Link--primary\n",
    "\n",
    "p_disc=soup.find_all('p',class_='f5 color-fg-muted mb-0 mt-1')\n",
    "#This will give the list of all the p tags in the webpage whose class is f5 color-text-secondary mb-0 mt-1\n",
    "print(p_disc)\n",
    "\n",
    "p_link=soup.find_all('a',class_='no-underline flex-1 d-flex flex-column')\n",
    "print(p_link)\n",
    "#This will give the list of all the a tags in the webpage whose class is no-underline flex-1 d-flex flex-column\n",
    "print(\"https://github.com\"+p_link[0]['href'])\n",
    "#This will give the link of the first topic in the webpage\n",
    "#['href'] is used to get the link of the tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Now we will create a list of all the topics,description & their link in the webpage\n",
    "topic_name=[]\n",
    "topic_disc=[]\n",
    "topic_link=[]\n",
    "for i in range(30):\n",
    "    topic_name.append(p_name[i].text.strip())\n",
    "    topic_disc.append(p_disc[i].text.strip())\n",
    "    topic_link.append(\"https://github.com\"+p_link[i]['href'])\n",
    "print(topic_name)\n",
    "print(topic_disc)\n",
    "print(topic_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will create a dataframe of all the topics,description & their link in the webpage\n",
    "import pandas as pd\n",
    "topics_df=pd.DataFrame({})\n",
    "topics_df['Topic Name']=topic_name\n",
    "topics_df['Topic Description']=topic_disc\n",
    "topics_df['Topic Link']=topic_link\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df.to_csv('gitHub.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_page_url=topic_link[0]\n",
    "response=requests.get(topic_page_url)\n",
    "response.status_code\n",
    "html_indi_content=response.text\n",
    "soup=BeautifulSoup(html_indi_content,'html.parser')\n",
    "#Repeating the same steps as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_tags=soup.find_all('h3',class_='f3 color-fg-muted text-normal lh-condensed')\n",
    "user_name=repo_tags[0].find_all('a')[0].text.strip() #repo_tags[0] is used to get the first repo and find_all('a')[0] is used to get the first username\n",
    "repo_name=repo_tags[0].find_all('a')[1].text.strip() #repo_tags[0] is used to get the first repo and find_all('a')[1] is used to get the first repo name\n",
    "repo_link=\"https://github.com\"+repo_tags[0].find_all('a')[1]['href'] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars_tags=soup.find_all('span',class_='Counter js-social-count')\n",
    "text=stars_tags[0].text.strip()\n",
    "def star_count(text):\n",
    "    if text[-1]=='k':\n",
    "        stars=int(float(text[:-1])*1000)\n",
    "    else:\n",
    "        stars=int(text)\n",
    "    return stars\n",
    "\n",
    "#This function is used to get the stars of the repo in integer format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repo_information(main_tag,main_star_tag):\n",
    "    a_tags=main_tag.find_all('a')\n",
    "    user_name=a_tags[0].text.strip()\n",
    "    repo_name=a_tags[1].text.strip()\n",
    "    repo_link=\"https://github.com\"+a_tags[1]['href']\n",
    "    stars=star_count(main_star_tag.text.strip())\n",
    "    return user_name,repo_name,repo_link,stars\n",
    "#Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_information(repo_tags[0],stars_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_username=[]\n",
    "repo_name=[]\n",
    "repo_link=[]\n",
    "repo_stars=[]\n",
    "for i in range(20):\n",
    "    repo_info=repo_information(repo_tags[i],stars_tags[i])\n",
    "    repo_username.append(repo_info[0])\n",
    "    repo_name.append(repo_info[1])\n",
    "    repo_link.append(repo_info[2])\n",
    "    repo_stars.append(repo_info[3])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Username':repo_username,'Repository Name':repo_name,'Repository Link':repo_link,'Stars':repo_stars})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will create a function to get the information of all repositories in each topic in the webpage\n",
    "def get_topic(url):\n",
    "    response=requests.get(url)\n",
    "    if response.status_code!=200:\n",
    "        raise Exception('Failed to load page {}'.format(url))\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    return soup\n",
    "\n",
    "def repo_information(main_tag,main_star_tag):\n",
    "    a_tags=main_tag.find_all('a')\n",
    "    user_name=a_tags[0].text.strip()\n",
    "    repo_name=a_tags[1].text.strip()\n",
    "    repo_link=\"https://github.com\"+a_tags[1]['href']\n",
    "    stars=star_count(main_star_tag.text.strip())\n",
    "    return user_name,repo_name,repo_link,stars\n",
    "\n",
    "def topic_repo(soup):\n",
    "    repo_tags=soup.find_all('h3',class_='f3 color-fg-muted text-normal lh-condensed')\n",
    "    stars_tags=soup.find_all('span',class_='Counter js-social-count')\n",
    "    topic_repos={\n",
    "        'Username':[],\n",
    "        'Repository Name':[],\n",
    "        'Repository Link':[],\n",
    "        'Stars':[]\n",
    "}\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info=repo_information(repo_tags[i],stars_tags[i])\n",
    "        topic_repos['Username'].append(repo_info[0])\n",
    "        topic_repos['Repository Name'].append(repo_info[1])\n",
    "        topic_repos['Repository Link'].append(repo_info[2])\n",
    "        topic_repos['Stars'].append(repo_info[3])\n",
    "    return pd.DataFrame(topic_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_repo(get_topic(topic_link[6]))\n",
    "#This will give the information of the third topic in the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def scrape_topic_csv(topic_url,path):\n",
    "    if os.path.exists(path):\n",
    "        print('File {} already exists'.format(path))\n",
    "        return\n",
    "    topic_df=topic_repo(get_topic(topic_url))\n",
    "    topic_df.to_csv(path,index=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topics_url='https://github.com/topics'\n",
    "    response=requests.get(topics_url)\n",
    "    if response.status_code!=200:\n",
    "        raise Exception('Failed to load page {}'.format(topics_url))\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    p_name=soup.find_all('p',class_='f3 lh-condensed mb-0 mt-1 Link--primary')\n",
    "    p_disc=soup.find_all('p',class_='f5 color-fg-muted mb-0 mt-1')\n",
    "    p_link=soup.find_all('a',class_='no-underline flex-1 d-flex flex-column')\n",
    "    topic_name=[]\n",
    "    topic_disc=[]\n",
    "    topic_link=[]\n",
    "    for i in range(len(p_name)):\n",
    "        topic_name.append(p_name[i].text.strip())\n",
    "        topic_disc.append(p_disc[i].text.strip())\n",
    "        topic_link.append(\"https://github.com\"+p_link[i]['href'])\n",
    "    topics_dictt={\n",
    "        'Topic Name':topic_name,\n",
    "        'Topic Description':topic_disc,\n",
    "        'Topic Link':topic_link\n",
    "\n",
    "    }\n",
    "    return pd.DataFrame(topics_dictt)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "def scrape_topics_repos():\n",
    "    topics_df=scrape_topics()\n",
    "    os.makedirs('Topics',exist_ok=True)\n",
    "    for index,row in topics_df.iterrows():\n",
    "        print('Scraping {} topic'.format(row['Topic Name']))\n",
    "        scrape_topic_csv(row['Topic Link'],'Topics/{}.csv'.format(row['Topic Name']))\n",
    "scrape_topics_repos()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
